{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bf3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd55d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, preprocess, _ = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\",\n",
    "    pretrained=\"laion2b_s34b_b79k\"\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828681f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_single_image(path: str) -> np.ndarray:\n",
    "    # 1) Load image as RGB\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "    img_tensor = preprocess(img).unsqueeze(0)  \n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode_image(img_tensor)   \n",
    "        emb = emb.float()                    \n",
    "\n",
    "    emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    vec = emb.squeeze(0).cpu().numpy()    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ff2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def embed_many_images(paths: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "    all_vecs = []\n",
    "\n",
    "    for i in range(0, len(paths), batch_size):\n",
    "        batch_paths = paths[i:i+batch_size]\n",
    "        imgs = []\n",
    "\n",
    "        for p in batch_paths:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            imgs.append(preprocess(img))\n",
    "\n",
    "        batch_tensor = torch.stack(imgs, dim=0).to(device)  # (B, 3, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb = model.encode_image(batch_tensor).float()  # (B, D)\n",
    "            emb = emb / emb.norm(dim=-1, keepdim=True)      # normalize per row\n",
    "\n",
    "        all_vecs.append(emb.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"/Users/alakarthika/Documents/Personal_Projects/WeakHero/assets/files/images.csv\")   \n",
    "paths = df[\"path\"].tolist()\n",
    "E = embed_many_images(paths)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70025a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/Users/alakarthika/Documents/Personal_Projects/WeakHero/assets/files/embeddings\", exist_ok=True)\n",
    "np.save(\"/Users/alakarthika/Documents/Personal_Projects/WeakHero/assets/files/embeddings/images.npy\", E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2853f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "899e0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "514abc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_list(texts, model, tokenizer, device=\"cpu\"):\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(texts).to(device)\n",
    "        emb = model.encode_text(tokens).float()\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    return emb.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927fc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_characters(df):\n",
    "    out = {}\n",
    "    for _, row in df.iterrows():\n",
    "        prompts = [str(row[c]) for c in df.columns if c.startswith(\"prompt_\") and pd.notna(row[c])]\n",
    "        embs = encode_text_list(prompts, model, tokenizer, device)\n",
    "        avg = embs.mean(axis=0)\n",
    "        avg /= np.linalg.norm(avg)\n",
    "        out[row[\"id\"]] = avg.tolist()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99101938",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = pd.read_csv(\"/Users/alakarthika/Documents/Personal_Projects/WeakHero/characters.csv\")\n",
    "char_embeds = embed_characters(chars)\n",
    "\n",
    "with open(\"/Users/alakarthika/Documents/Personal_Projects/WeakHero/assets/files/embeddings/char_embeds.json\",\"w\") as f:\n",
    "    json.dump(char_embeds,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a9606c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"/Users/alakarthika/Documents/Personal_Projects/WeakHero/explain_labels.csv\")\n",
    "label_vecs = encode_text_list(labels[\"label\"].tolist(), model, tokenizer, device)\n",
    "\n",
    "label_embeds = {lab: vec.tolist() for lab, vec in zip(labels[\"label\"], label_vecs)}\n",
    "with open(\"/Users/alakarthika/Documents/Personal_Projects/WeakHero/assets/files/embeddings/label_embeds.json\",\"w\") as f:\n",
    "    json.dump(label_embeds,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weakhero1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
